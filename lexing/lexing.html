<!DOCTYPE html>
<head>
    <title>Example</title>
    <link rel="stylesheet" href="../_common/tachyons.min.css">

    <style>
        pre {
            margin: 0;
        }

        pre span {
            border-radius: 3px;
        }
    </style>
</head>

<body class="ma3 sans-serif">
    <h1>Lexing</h1>

    <p>Lexing is the first stage of compilation. At this stage the source code is split into <i>tokens</i>, which represent each atomic part of the program. These atomic parts are things such as symbols, numbers, names of things, and the like.</p>

    <p>The change from source code to tokens is in principle small. Both source code and tokens are sequences of some sort of symbols, but moving from text characters to symbols specific to the language is very helpful, as it simplifies the work the compiler must do later.</p>

    <p>A lexer traverses the source code and attempts to match "words" to "known things". For instance, a lexer may match the text "42" to an integer literal.</p>
    <p></p>


    <p>You can use this interactable to see how a lexer may split source code into tokens.</p>
    <div class="pa1 ba relative overflow-y-scroll">
        <pre id="highlights" class="absolute"></pre>
        <pre id="editor" class="relative w6 h5" contenteditable="true">// TODO: Maybe change these to use mu_Vec2.
static float START_ANGLES[4] = {     PI, 3*PI/2,    0, PI/2 };
static float END_ANGLES[4]   = { 3*PI/2,      0, PI/2,   PI };

int getQuadrantXY(int dx, int dy) {
    if (dx < 0) {
        if (dy < 0) {
            return 0;
        } else {
            return 3;
        }
    } else {
        if (dy < 0) {
            return 1;
        } else {
            return 2;
        }
    }
}</pre>
    </div>
    <p id="results"></p>

    <!--
    NOTE(Zelaven): I consider the text above the interablcible to be done for now.
    

		Producing a string slice by looking for the end of the token. What ends the token depends on what character started the token.
		Emphasize the character-by-character nature of the lexer.
		A token is a pattern of characters. A regex is a compact description of such a pattern and therefore often appears in lexers.



    TODO:
    - Explain the ideas of how to match things.
    - Mention regex.
    - Show and explain a couple of key snippets.
    - Conclusion
    -- Refer to the full code
    
    -->
    <p>
Implementing a lexer requires that you iterate over the characters of the source code.
There are two primary observations that your code must do as it passes over the code.
First, when a new token starts, it must determine what kind of token it is.
Secondly, it must determine where the end of the token is, hence why it needs to know what kind of token it is.
Finding the boundaries of a token string is commonly called "matching" or "to match" a token, a term related to regular expressions.
    </p><p>
The initial state of the lexer is at the beginning of the input, and starting a new token.
Source code contains many characters that exist for readability, rather than for semantic reasons, such as extra whitespace characters and code comments. These characters are commonly skipped by the lexer.
This can in some form be considered to be a match with a special rule that doesn't produce any token.
    </p>
    <p>
Below is a snippet from the lexer used for the interactable element above.
The strategy employed is to trim away the whitespace and comments, and then try to match each of the token types one at a time.
    </p>
    <pre>
const tokens = [];    
while (!eof) {
    consumeWhitespaceAndComments();
    if (eof) {
        break;
    }

    const punctuator = lexPunctuator();
    if (punctuator) {
        tokens.push(punctuator);
        continue;
    }

    const char = lexCharacterConstant();
    if (char) {
        tokens.push(char);
        continue;
    }

    const str = lexStringLiteral();
    if (str) {
        tokens.push(str);
        continue;
    }

    /* ... */
}
    </pre>
    <p>
The process of matching strings can be done either by hand-coding the mechanism, or by using regular expressions.
For those unfamiliar with regular expressions, they can be considered a domain-specific language that expresses what a "family" of strings "look like".
For instance, a regular expression can be used to specify what an integer literal token looks like.
    </p>
    <p>
First, this example showcases an example of hand-coded comparisons.
The lexPunctuator function simply compares tests the input against each punctuator in the list, and stops if it finds one that matches.
    </p>
    <pre>
// Punctuators must be sorted by length in order to ensure that e.g. "++" is
// not parsed as "+" "+".
const punctuators = [
    "%:%:",
    
    "<<=", ">>=", "...",
    
    "<:", ":>", "<%", "%>", "%:", "<<", ">>", "<=", ">=", "##", "->", "++",
    "--", "*=", "/=", "%=", "+=", "-=", "&=", "^=", "|=", "&&", "||", "==",
    "!=",
    
    "<", ">", ".", "&", "*", "+", "-", "~", "!", "/", "%", "^", "|", "?", ":",
    ";", "=", ",", "#", "[", "]", "(", ")", "{", "}",
];

function lexPunctuator() {
    for (const punctuator of punctuators) {
        if (nextIs(punctuator)) {
            const loc = consume(punctuator);
            return { type: "punctuator", loc: loc };
        }
    }
    return null;    
}
    </pre>
    <p>
The next example is lexCharacterConstact, which uses a regular expression to specify what a character constant token is.
When using regular expressions, the complexity lies in the specification of the regular expression, rather than in the complexity of the code.
Regular expressions are a powerful tool that is often used in lexers, and tools such as flex are built to generate efficient lexers based on regular expressions.
    </p>
    <pre>
const reCharConstant = /^L?'([^'\\\n]|\\['"?\\abfnrtv]|\\[0-7]{1,3}|\\x[0-9a-fA-F]+|\\(u[0-9a-fA-F]{4}|U[0-9a-fA-F]{8}))+'/;

function lexCharacterConstant() {
    const char = consumeIfMatch(reCharConstant);
    if (char) {
        return { type: "character", loc: char };
    }

    return null;
}
    </pre>
    <p>
While regular expressions can be very useful, they are not strictly necessary for writing a lexer.
The matching logic expressed by a regular expression can also be expressed in code, and a regular expression can be viewed as a more compact way to express the logic.
Whether or not regular expressions are used, and for what tokens, is a design decision that should be made in the context of the use case.
    </p>

    <br>

    <p>
Lexing is the process in which the compiler categorizes each piece of the input.
It keeps only the important information, and trims away whitespace and comments that aren't used later on.
Lexing can be done using handcrafted comparisons, regular expressions, or both to match characters in the input to produce tokens.
    </p>
    <p>
    <!-- TODO(Zelaven): Make this point to the main branch of the repository-->
    For the complete source code of the lexer used for the interactable element, please see <a href="https://github.com/HandmadeNetwork/explainers/blob/lexing/lexing/lexing.js">this file</a>.
    </p>
    <p></p>
    <p></p>
    <p></p>

    <script src="lexing.js"></script>
</body>
